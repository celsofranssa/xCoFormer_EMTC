name: BERT_TGT

text_encoder:
  _target_: source.encoder.BertEncoder.BertEncoder
  architecture: bert-base-uncased
  output_attentions: False
  pooling:
    _target_: source.pooling.NoPooling.NoPooling

label_encoder:
  _target_: source.encoder.BertEncoder.BertEncoder
  architecture: bert-base-uncased
  output_attentions: False
  pooling:
    _target_: source.pooling.NoPooling.NoPooling

hidden_size: 768

text_tokenizer:
  architecture: ${model.text_encoder.architecture}

label_tokenizer:
  architecture: ${model.label_encoder.architecture}

lr: 5e-5
text_lr: 5e-5
label_lr: 5e-5
base_lr: 5e-6
max_lr: 5e-3
weight_decay: 1e-2

tag_training: True
text_frequency_opt: 1
label_frequency_opt: 1

loss:
  _target_: source.loss.NTXentLoss.NTXentLoss
  params:
    name: NTXentLoss
    miner:
      epsilon: 0.1
    criterion:
      temperature: 0.07

metric:
  relevance_map:
    dir: ${data.dir}
    name: relevance_map